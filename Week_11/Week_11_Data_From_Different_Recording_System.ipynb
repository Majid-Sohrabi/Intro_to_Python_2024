{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8ShhvetFpA7"
   },
   "source": [
    "# Week 11: Reading Data for Different Recording System\n",
    "\n",
    "Source: [MNE-Python](https://mne.tools/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Data from MEG Device\n",
    "\n",
    "This section describes how to read data for various MEG manufacturers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEGIN/Elekta Neuromag VectorView and TRIUX (.fif)\n",
    "\n",
    "Neuromag Raw FIF files can be loaded using `mne.io.read_raw_fif()`.\n",
    "\n",
    "If the data were recorded with MaxShield on and have not been processed with MaxFilter, they may need to be loaded with `mne.io.read_raw_fif(..., allow_maxshield=True)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIL OPM (.bin)\n",
    "\n",
    "MEG data from the OPM system used by the FIL at UCL can be read with `mne.io.read_raw_fil()`. For related OPM processing methods, see Preprocessing optically pumped magnetometer (OPM) MEG data.\n",
    "\n",
    "### Artemis123 (.bin)\n",
    "\n",
    "MEG data from the Artemis123 system can be read with `mne.io.read_raw_artemis123()`.\n",
    "\n",
    "### 4-D Neuroimaging / BTI data (dir)\n",
    "\n",
    "MNE-Python provides `mne.io.read_raw_bti()` to read and convert 4D / BTI data. This reader function will by default replace the original channel names, typically composed of the letter `A` and the channel number with Neuromag. To import the data, the following input files are mandatory:\n",
    "\n",
    "* A data file (typically c,rfDC) containing the recorded MEG time series.\n",
    "\n",
    "* A hs_file containing the digitizer data.\n",
    "\n",
    "* A config file containing acquisition information and metadata.\n",
    "\n",
    "By default `mne.io.read_raw_bti()` assumes that these three files are located in the same folder.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>While reading the reference or compensation channels,\n",
    "          the compensation weights are currently not processed.\n",
    "          As a result, the :class:`mne.io.Raw` object and the corresponding fif\n",
    "          file does not include information about the compensation channels\n",
    "          and the weights to be applied to realize software gradient\n",
    "          compensation. If the data are saved in the Magnes system are already\n",
    "          compensated, there will be a small error in the forward calculations,\n",
    "          whose significance has not been evaluated carefully at this time.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTF data (dir)\n",
    "\n",
    "The function `mne.io.read_raw_ctf()` can be used to read CTF data.\n",
    "\n",
    "### CTF Polhemus data\n",
    "\n",
    "The function `mne.channels.read_dig_polhemus_isotrak()` can be used to read Polhemus data.\n",
    "\n",
    "### Applying software gradient compensation\n",
    "\n",
    "Since the software gradient compensation employed in CTF systems is a reversible operation, it is possible to change the compensation status of CTF data in the data files as desired. This section contains information about the technical details of the compensation procedure and a description of `mne.io.Raw.apply_gradient_compensation()`.\n",
    "\n",
    "The raw instances returned by `mne.io.read_raw_ctf()` contain several compensation matrices which are employed to suppress external disturbances with help of the reference channel data. The reference sensors are located further away from the brain than the helmet sensors and are thus measuring mainly the external disturbances rather than magnetic fields originating in the brain. Most often, a compensation matrix corresponding to a scheme nicknamed *Third-order gradient compensation* is employed.\n",
    "\n",
    "Let us assume that the data contain $n_1$ MEG\n",
    "sensor channels, $n_2$ reference sensor\n",
    "channels, and $n_3$ other channels.\n",
    "The data from all channels can be concatenated into a single vector\n",
    "\n",
    "\\begin{align}x = [x_1^T x_2^T x_3^T]^T\\ ,\\end{align}\n",
    "\n",
    "where $x_1$, $x_2$,\n",
    "and $x_3$ are the data vectors corresponding\n",
    "to the MEG sensor channels, reference sensor channels, and other\n",
    "channels, respectively. The data before and after compensation,\n",
    "denoted here by $x_{(0)}$ and $x_{(k)}$, respectively,\n",
    "are related by\n",
    "\n",
    "\\begin{align}x_{(k)} = M_{(k)} x_{(0)}\\ ,\\end{align}\n",
    "\n",
    "where the composite compensation matrix is\n",
    "\n",
    "\\begin{align}M_{(k)} = \\begin{bmatrix}\n",
    "                I_{n_1} & C_{(k)} & 0 \\\\\n",
    "                0 & I_{n_2} & 0 \\\\\n",
    "                0 & 0 & I_{n_3}\n",
    "                \\end{bmatrix}\\ .\\end{align}\n",
    "\n",
    "In the above, $C_{(k)}$ is a $n_1$ by $n_2$ compensation\n",
    "data matrix corresponding to compensation \"grade\" $k$.\n",
    "It is easy to see that\n",
    "\n",
    "\\begin{align}M_{(k)}^{-1} = \\begin{bmatrix}\n",
    "                I_{n_1} & -C_{(k)} & 0 \\\\\n",
    "                0 & I_{n_2} & 0 \\\\\n",
    "                0 & 0 & I_{n_3}\n",
    "                \\end{bmatrix}\\ .\\end{align}\n",
    "\n",
    "To convert from compensation grade $k$ to $p$ one\n",
    "can simply multiply the inverse of one compensate compensation matrix\n",
    "by another and apply the product to the data:\n",
    "\n",
    "\\begin{align}x_{(k)} = M_{(k)} M_{(p)}^{-1} x_{(p)}\\ .\\end{align}\n",
    "\n",
    "This operation is performed by :meth:`mne.io.Raw.apply_gradient_compensation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ricoh/KIT MEG system data (.con/.sqd)\n",
    "\n",
    "MNE-Python includes the `mne.io.read_raw_kit()` and `mne.read_epochs_kit()` to read and convert Ricoh/KIT MEG data.\n",
    "\n",
    "**Note (Channel Naming)**:\n",
    "```\n",
    "In MNE 0.21 This reader function will by default replace the original channel names, which typically with index starting with zero, with ones with an index starting with one. In 0.22 it will use native names when possible. Use the standardize_names argument to control this behavior.\n",
    "```\n",
    "\n",
    "To import continuous data, only the input .sqd or .con file is needed. For epochs, an Nx3 matrix containing the event number/corresponding trigger value in the third column is needed.\n",
    "\n",
    "The following input files are optional:\n",
    "\n",
    "* A KIT marker file (mrk file) or an array-like containing the locations of the HPI coils in the MEG device coordinate system. These data are used together with the elp file to establish the coordinate transformation between the head and device coordinate systems.\n",
    "\n",
    "* A Polhemus points file (elp file) or an array-like containing the locations of the fiducials and the head-position indicator (HPI) coils. These data are usually given in the Polhemus head coordinate system.\n",
    "\n",
    "* A Polhemus head shape data file (hsp file) or an array-like containing locations of additional points from the head surface. These points must be given in the same coordinate system as that used for the elp file.\n",
    "\n",
    "Modern Ricoh systems may encode this information it the file itself, in which case `mrk`, `elp`, and `hsp` can all be `None` and the data will be read from the file itself.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The output fif file will use the Neuromag head coordinate system convention,\n",
    "   see `coordinate_systems`. A coordinate transformation between the\n",
    "   Polhemus head coordinates and the Neuromag head coordinates is included.</p></div>\n",
    "\n",
    "By default, KIT-157 systems assume the first 157 channels are the MEG channels,\n",
    "the next 3 channels are the reference compensation channels, and channels 160\n",
    "onwards are designated as miscellaneous input channels (MISC 001, MISC 002,\n",
    "etc.).\n",
    "By default, KIT-208 systems assume the first 208 channels are the MEG channels,\n",
    "the next 16 channels are the reference compensation channels, and channels 224\n",
    "onwards are designated as miscellaneous input channels (MISC 001, MISC 002,\n",
    "etc.).\n",
    "\n",
    "In addition, it is possible to synthesize the digital trigger channel (STI 014)\n",
    "from available analog trigger channel data by specifying the following\n",
    "parameters:\n",
    "\n",
    "- A list of trigger channels (stim) or default triggers with order: '<' | '>'\n",
    "  Channel-value correspondence when converting KIT trigger channels to a\n",
    "  Neuromag-style stim channel. By default, we assume the first eight\n",
    "  miscellaneous channels are trigger channels. For '<', the largest values are\n",
    "  assigned to the first channel (little endian; default). For '>', the largest\n",
    "  values are assigned to the last channel (big endian). Can also be specified\n",
    "  as a list of trigger channel indexes.\n",
    "- The trigger channel slope (slope) : '+' | '-'\n",
    "  How to interpret values on KIT trigger channels when synthesizing a\n",
    "  Neuromag-style stim channel. With '+', a positive slope (low-to-high)\n",
    "  is interpreted as an event. With '-', a negative slope (high-to-low)\n",
    "  is interpreted as an event.\n",
    "- A stimulus threshold (stimthresh) : float\n",
    "  The threshold level for accepting voltage changes in KIT trigger\n",
    "  channels as a trigger event.\n",
    "\n",
    "The synthesized trigger channel data value at sample $k$ will\n",
    "be:\n",
    "\n",
    "\\begin{align}s(k) = \\sum_{p = 1}^n {t_p(k) 2^{p - 1}}\\ ,\\end{align}\n",
    "\n",
    "where $t_p(k)$ are the thresholded\n",
    "from the input channel data d_p(k):\n",
    "\n",
    "\\begin{align}t_p(k) = \\Bigg\\{ \\begin{array}{l}\n",
    "                 0 \\text{  if  } d_p(k) \\leq t\\\\\n",
    "                 1 \\text{  if  } d_p(k) > t\n",
    "             \\end{array}\\ .\\end{align}\n",
    "\n",
    "The threshold value $t$ can\n",
    "be adjusted with the ``stimthresh`` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FieldTrip MEG/EEG data (.mat)\n",
    "\n",
    "MNE-Python includes :func:`mne.io.read_raw_fieldtrip`, :func:`mne.read_epochs_fieldtrip` and :func:`mne.read_evoked_fieldtrip` to read data coming from FieldTrip.\n",
    "\n",
    "The data is imported directly from a ``.mat`` file.\n",
    "\n",
    "The ``info`` parameter can be explicitly set to ``None``. The import functions will still work but:\n",
    "\n",
    "1. All channel locations will be in head coordinates.\n",
    "2. Channel orientations cannot be guaranteed to be accurate.\n",
    "3. All channel types will be set to generic types.\n",
    "\n",
    "This is probably fine for anything that does not need that information, but if you intent to do things like interpolation of missing channels, source analysis or look at the RMS pairs of planar gradiometers, you most likely run into problems.\n",
    "\n",
    "It is **highly recommended** to provide the `info` parameter as well. The `info` dictionary can be extracted by loading the original raw data file with the corresponding MNE-Python functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = mne.io.read_raw_fiff('original_data.fif', preload=False)\n",
    "original_info = original_data.info\n",
    "data_from_ft = mne.read_evoked_fieldtrip('evoked_data.mat', original_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imported data can have less channels than the original data. Only the information for the present ones is extracted from the `info` dictionary.\n",
    "\n",
    "As of version 0.17, importing FieldTrip data has been tested on a variety of systems with the following results:\n",
    "\n",
    "\n",
    "**Correct the Table** ###################\n",
    "| System | Read Raw Data | Read Epoched Data | Read Evoked Data |\n",
    "| ------ | ------------- | ----------------- | ---------------- |\n",
    "| BTI | Works | Untested | Untested |\n",
    "| CNT |Data imported as microvolts.  Otherwise fine.| Data imported as microvolts. Otherwise fine. | Data imported as microvolts. Otherwise fine. |\n",
    "\n",
    "**Correct the Table** ####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating MNE data structures from arbitrary data (from memory)\n",
    "\n",
    "Arbitrary (e.g., simulated or manually read in) raw data can be constructed from memory by making use of `mne.io.RawArray`, `mne.EpochsArray` or `mne.EvokedArray` in combination with `mne.create_info()`.\n",
    "\n",
    "This functionality is illustrated in `Creating MNE-Python data structures from scratch`. Using 3rd party libraries such as `NEO` in combination with these functions abundant electrophysiological file formats can be easily loaded into MNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Data from EEG Device\n",
    "\n",
    "MNE includes various functions and utilities for reading EEG data and electrode locations.\n",
    "\n",
    "### BrainVision (.vhdr, .vmrk, .eeg)\n",
    "\n",
    "The BrainVision file format consists of three separate files:\n",
    "\n",
    "1. A text header file (``.vhdr``) containing meta data.\n",
    "2. A text marker file (``.vmrk``) containing information about events in the\n",
    "   data.\n",
    "3. A binary data file (``.eeg``) containing the voltage values of the EEG.\n",
    "\n",
    "Both text files are based on the [INI format](https://en.wikipedia.org/wiki/INI_file)\n",
    "consisting of\n",
    "\n",
    "* sections marked as ``[square brackets]``,\n",
    "* comments marked as ``; comment``,\n",
    "* and key-value pairs marked as ``key=value``.\n",
    "\n",
    "Brain Products provides documentation for their core BrainVision file format.\n",
    "The format specification is hosted on the\n",
    "[Brain Products website](https://www.brainproducts.com/support-resources/brainvision-core-data-format-1-0/).\n",
    "\n",
    "BrainVision EEG files can be read using `mne.io.read_raw_brainvision`,\n",
    "passing the ``.vhdr`` header file as the argument.\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Renaming BrainVision files can be problematic due to their\n",
    "             multi-file structure. See this\n",
    "             [example](https://mne.tools/mne-bids/stable/auto_examples/rename_brainvision_files.html#sphx-glr-auto-examples-rename-brainvision-files-py)\n",
    "             for instructions.</p></div>\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>For *writing* BrainVision files, take a look at the :mod:`mne.export`\n",
    "          module, which used the [pybv](https://pypi.org/project/pybv/) Python\n",
    "          package.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European data format (.edf)\n",
    "\n",
    "[EDF](http://www.edfplus.info/specs/edf.html) and\n",
    "[EDF+](http://www.edfplus.info/specs/edfplus.html) files can be read using\n",
    "`mne.io.read_raw_edf`. Both variants are 16-bit formats.\n",
    "\n",
    "EDF+ files may contain annotation channels which can be used to store trigger\n",
    "and event information. These annotations are available in ``raw.annotations``.\n",
    "\n",
    "Writing EDF files is not supported natively yet. [This gist](https://gist.github.com/skjerns/bc660ef59dca0dbd53f00ed38c42f6be)_ or\n",
    "[MNELAB](https://github.com/cbrnr/mnelab) (both of which use\n",
    "[pyedflib](https://github.com/holgern/pyedflib) under the hood) can be used\n",
    "to export any `mne.io.Raw` object to EDF/EDF+/BDF/BDF+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioSemi data format (.bdf)\n",
    "\n",
    "The `BDF format` is a 24-bit variant of the EDF format used by EEG systems manufactured by BioSemi. It can be imported with `mne.io.read_raw_bdf()`.\n",
    "\n",
    "BioSemi amplifiers do not perform “common mode noise rejection” automatically. The signals in the EEG file are the voltages between each electrode and the CMS active electrode, which still contain some CM noise (50 Hz, ADC reference noise, etc.). The [BioSemi FAQ](https://www.biosemi.com/faq/cms&drl.htm) provides more details on this topic. Therefore, it is advisable to choose a reference (e.g., a single channel like Cz, average of linked mastoids, average of all electrodes, etc.) after importing BioSemi data to avoid losing signal information. The data can be re-referenced later after cleaning if desired.\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Data samples in a BDF file are represented in a 3-byte\n",
    "             (24-bit) format. Since 3-byte raw data buffers are not presently\n",
    "             supported in the FIF format, these data will be changed to 4-byte\n",
    "             integers in the conversion.</p></div>\n",
    "             \n",
    "### General data format (.gdf)\n",
    "\n",
    "GDF files can be read using `mne.io.read_raw_gdf`.\n",
    "\n",
    "[GDF (General Data Format)](https://arxiv.org/abs/cs/0608052) is a flexible\n",
    "format for biomedical signals that overcomes some of the limitations of the\n",
    "EDF format. The original specification (GDF v1) includes a binary header\n",
    "and uses an event table. An updated specification (GDF v2) was released in\n",
    "2011 and adds fields for additional subject-specific information (gender,\n",
    "age, etc.) and allows storing several physical units and other properties.\n",
    "Both specifications are supported by MNE.\n",
    "\n",
    "### Neuroscan CNT (.cnt)\n",
    "\n",
    "CNT files can be read using `mne.io.read_raw_cnt`.\n",
    "Channel locations can be read from a montage or the file header. If read\n",
    "from the header, the data channels (channels that are not assigned to EOG, ECG,\n",
    "EMG or MISC) are fit to a sphere and assigned a z-value accordingly. If a\n",
    "non-data channel does not fit to the sphere, it is assigned a z-value of 0.\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Reading channel locations from the file header may be dangerous, as the\n",
    "    x_coord and y_coord in the ELECTLOC section of the header do not necessarily\n",
    "    translate to absolute locations. Furthermore, EEG electrode locations that\n",
    "    do not fit to a sphere will distort the layout when computing the z-values.\n",
    "    If you are not sure about the channel locations in the header, using a\n",
    "    montage is encouraged.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EGI simple binary (.egi)\n",
    "\n",
    "EGI simple binary files can be read using `mne.io.read_raw_egi`.\n",
    "EGI raw files are simple binary files with a header and can be exported by the\n",
    "EGI Netstation acquisition software.\n",
    "\n",
    "\n",
    "\n",
    "### EGI MFF (.mff)\n",
    "\n",
    "EGI MFF files can be read with `mne.io.read_raw_egi`.\n",
    "\n",
    "\n",
    "\n",
    "### EEGLAB files (.set, .fdt)\n",
    "\n",
    "EEGLAB .set files (which sometimes come with a separate .fdt file) can be read\n",
    "using `mne.io.read_raw_eeglab` and `mne.read_epochs_eeglab`.\n",
    "\n",
    "\n",
    "\n",
    "### Nicolet (.data)\n",
    "\n",
    "These files can be read with `mne.io.read_raw_nicolet`.\n",
    "\n",
    "\n",
    "\n",
    "### eXimia EEG data (.nxe)\n",
    "\n",
    "EEG data from the Nexstim eXimia system can be read with `mne.io.read_raw_eximia`.\n",
    "\n",
    "\n",
    "\n",
    "### Persyst EEG data (.lay, .dat)\n",
    "\n",
    "EEG data from the Persyst system can be read with `mne.io.read_raw_persyst`.\n",
    "\n",
    "Note that subject metadata may not be properly imported because Persyst\n",
    "sometimes changes its specification from version to version. Please let us know\n",
    "if you encounter a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nihon Kohden EEG data (.eeg, .21e, .pnt, .log)\n",
    "\n",
    "EEG data from the Nihon Kohden (NK) system can be read using the `mne.io.read_raw_nihon` function.\n",
    "\n",
    "Files with the following extensions will be read:\n",
    "\n",
    "- The `.eeg` file contains the actual raw EEG data.\n",
    "- The `.pnt` file contains metadata related to the recording such as the measurement date.\n",
    "- The `.log` file contains annotations for the recording.\n",
    "- The `.21e` file contains channel and electrode information.\n",
    "\n",
    "Reading `.11d`, `.cmt`, `.cn2`, and `.edf` files is currently not supported.\n",
    "\n",
    "Note that not all subject metadata may be properly read because NK changes the specification sometimes from version to version. Please let us know if you encounter a problem.\n",
    "\n",
    "### XDF data (.xdf, .xdfz)\n",
    "\n",
    "MNE-Python does not support loading [XDF](https://github.com/sccn/xdf/wiki/Specifications) files out of the box, because the inherent flexibility of the XDF format makes it difficult to provide a one-size-fits-all function. For example, XDF supports signals from various modalities recorded with different sampling rates. However, it is relatively straightforward to import only a specific stream (such as EEG signals) using the [pyxdf](https://github.com/xdf-modules/pyxdf) package.\n",
    "\n",
    "A more sophisticated version, which supports selection of specific streams as well as converting marker streams into annotations, is available in [MNELAB](https://github.com/cbrnr/mnelab). If you want to use this functionality in a script, MNELAB records its history (View - History), which contains all commands required to load an XDF file after successfully loading that file with the graphical user interface.\n",
    "\n",
    "\n",
    "### Setting EEG references\n",
    "\n",
    "The preferred method for applying an EEG reference in MNE is `mne.set_eeg_reference`, or equivalent instance methods like `raw.set_eeg_reference()`. By default, the data are assumed to already be properly referenced.\n",
    "\n",
    "\n",
    "### Reading electrode locations and head shapes for EEG recordings\n",
    "\n",
    "Some EEG formats (e.g., EGI, EDF/EDF+, BDF) contain neither electrode locations nor head shape digitization information. Therefore, this information has to be provided separately. For that purpose, all raw instances have a `mne.io.Raw.set_montage` method to set electrode locations.\n",
    "\n",
    "When using locations of fiducial points, the digitization data are converted to the MEG head coordinate system employed in the MNE software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importing Data from fNIRS Devices\n",
    "\n",
    "fNIRS devices consist of two kinds of optodes: light sources (AKA \"emitters\" or\n",
    "\"transmitters\") and light detectors (AKA \"receivers\"). Channels are defined as\n",
    "source-detector pairs, and channel locations are defined as the midpoint\n",
    "between source and detector.\n",
    "\n",
    "MNE-Python provides functions for reading fNIRS data and optode locations from\n",
    "several file formats. Regardless of the device manufacturer or file format,\n",
    "MNE-Python's fNIRS functions will internally store the measurement data and its\n",
    "metadata in the same way (e.g., data values are always converted into SI\n",
    "units). Supported measurement types include amplitude, optical density,\n",
    "oxyhaemoglobin concentration, and deoxyhemoglobin concentration (for continuous\n",
    "wave fNIRS), and additionally AC amplitude and phase (for\n",
    "frequency domain fNIRS).\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>MNE-Python stores metadata internally with a specific structure,\n",
    "             and internal functions expect specific naming conventions.\n",
    "             Manual modification of channel names and metadata\n",
    "             is not recommended.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Data\n",
    "\n",
    "### SNIRF (.snirf)\n",
    "\n",
    "The Shared Near Infrared Spectroscopy Format\n",
    "([SNIRF](https://github.com/fNIRS/snirf/blob/master/snirf_specification.md)_)\n",
    "is designed by the fNIRS community in an effort to facilitate\n",
    "sharing and analysis of fNIRS data. And is the official format of the\n",
    "Society for functional near-infrared spectroscopy (SfNIRS).\n",
    "The manufacturers Gowerlabs, NIRx, Kernel, Artinis, and Cortivision\n",
    "export data in the SNIRF format, and these files can be imported in to MNE.\n",
    "SNIRF is the preferred format for reading data in to MNE-Python.\n",
    "Data stored in the SNIRF format can be read in\n",
    "using `mne.io.read_raw_snirf`.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The SNIRF format has provisions for many different types of fNIRS\n",
    "          recordings. MNE-Python currently only supports reading continuous\n",
    "          wave or haemoglobin data stored in the .snirf format.</p></div>\n",
    "          \n",
    "          \n",
    "#### Specifying the coordinate system\n",
    "\n",
    "There are a variety of coordinate systems used to specify the location of\n",
    "sensors (see `tut-source-alignment` for details). Where possible the\n",
    "coordinate system will be determined automatically when reading a SNIRF file.\n",
    "However, sometimes this is not possible and you must manually specify the\n",
    "coordinate frame the optodes are in. This is done using the ``optode_frame``\n",
    "argument when loading data.\n",
    "\n",
    "\n",
    "**Correct the Table** #########################\n",
    "=======  ==================  =================\n",
    "Vendor   Model               ``optode_frame``\n",
    "=======  ==================  =================\n",
    "NIRx     ICBM-152 MNI        mri\n",
    "Kernel   ICBM 2009b          mri\n",
    "=======  ==================  =================\n",
    "**Correct the Table** #########################\n",
    "\n",
    "The coordinate system is automatically detected for Gowerlabs SNIRF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Wave Devices\n",
    "\n",
    "### NIRx (directory or hdr)\n",
    "\n",
    "NIRx produce continuous wave fNIRS devices. NIRx recordings can be read in using `mne.io.read_raw_nirx`.\n",
    "The NIRx device stores data directly to a directory with multiple file types, MNE-Python extracts the appropriate information from each file. MNE-Python only supports NIRx files recorded with NIRStar version 15.0 and above and Aurora version 2021 and above. MNE-Python supports reading data from NIRScout and NIRSport devices.\n",
    "\n",
    "### Hitachi (.csv)\n",
    "\n",
    "Hitachi produce continuous wave fNIRS devices. Hitachi fNIRS recordings can be read using `mne.io.read_raw_hitachi`. No optode information is stored so you'll need to set the montage manually, see the Notes section of `mne.io.read_raw_hitachi`.\n",
    "\n",
    "## Frequency Domain Devices\n",
    "\n",
    "\n",
    "### BOXY (.txt)\n",
    "\n",
    "BOXY recordings can be read in using `mne.io.read_raw_boxy`. The BOXY software and ISS Imagent I and II devices are frequency domain systems that store data in a single ``.txt`` file containing what they call (with MNE-Python's name for that type of data in parens):\n",
    "\n",
    "- **DC**\n",
    "    All light collected by the detector (``fnirs_cw_amplitude``)\n",
    "- **AC**\n",
    "    High-frequency modulated light intensity (``fnirs_fd_ac_amplitude``)\n",
    "- **Phase**\n",
    "    Phase of the modulated light (``fnirs_fd_phase``)\n",
    "\n",
    "DC data is stored as the type ``fnirs_cw_amplitude`` because it collects both the modulated and any unmodulated light, and hence is analogous to what is collected by continuous wave systems such as NIRx. This helps with conformance to SNIRF standard types.\n",
    "\n",
    "These raw data files can be saved by the acquisition devices as parsed or unparsed ``.txt`` files, which affects how the data in the file is organised. MNE-Python will read either file type and extract the raw DC, AC, and Phase data. If triggers are sent using the ``digaux`` port of the recording hardware, MNE-Python will also read the ``digaux`` data and create annotations for any triggers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Import\n",
    "\n",
    "### Loading legacy data in CSV or TSV format\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>This method is not supported and users are discouraged to use it.\n",
    "             You should convert your data to the\n",
    "             [SNIRF](https://github.com/fNIRS/snirf) format using the tools\n",
    "             provided by the Society for functional Near-Infrared Spectroscopy,\n",
    "             and then load it using :func:`mne.io.read_raw_snirf`.</p></div>\n",
    "\n",
    "fNIRS measurements may be stored in a non-standardised format that is not supported by MNE-Python and cannot be converted easily into SNIRF. This legacy data is often in CSV or TSV format, we show here a way to load it even though it is not officially supported by MNE-Python due to the lack of standardisation of the file format (the naming and ordering of channels, the type and scaling of data, and specification of sensor positions varies between each vendor). You will likely have to adapt this depending on the system from which your CSV originated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate an example CSV file which will then be loaded in to MNE-Python. This step would be skipped if you have actual data you wish to load. We simulate 16 channels with 100 samples of data and save this to a file called fnirs.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.random.normal(size=(16, 100))).to_csv(\"fnirs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the example CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"fnirs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the metadata must be specified manually as the CSV file does not\n",
    "contain information about channel names, types, sample rate etc.\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>In MNE-Python the naming of channels MUST follow the structure\n",
    "             ``S#_D# type`` where # is replaced by the appropriate source and\n",
    "             detector numbers and type is either ``hbo``, ``hbr`` or the\n",
    "             wavelength.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names = [\n",
    "    \"S1_D1 hbo\",\n",
    "    \"S1_D1 hbr\",\n",
    "    \"S2_D1 hbo\",\n",
    "    \"S2_D1 hbr\",\n",
    "    \"S3_D1 hbo\",\n",
    "    \"S3_D1 hbr\",\n",
    "    \"S4_D1 hbo\",\n",
    "    \"S4_D1 hbr\",\n",
    "    \"S5_D2 hbo\",\n",
    "    \"S5_D2 hbr\",\n",
    "    \"S6_D2 hbo\",\n",
    "    \"S6_D2 hbr\",\n",
    "    \"S7_D2 hbo\",\n",
    "    \"S7_D2 hbr\",\n",
    "    \"S8_D2 hbo\",\n",
    "    \"S8_D2 hbr\",\n",
    "]\n",
    "ch_types = [\n",
    "    \"hbo\",\n",
    "    \"hbr\",\n",
    "    \"hbo\",\n",
    "    \"hbr\",\n",
    "    \"hbo\",\n",
    "    \"hbr\",\n",
    "    \"hbo\",\n",
    "    \"hbr\",\n",
    "    \"hbo\",\n",
    "    \"hbr\",\n",
    "    \"hbo\",\n",
    "    \"hbr\",\n",
    "    \"hbo\",\n",
    "    \"hbr\",\n",
    "    \"hbo\",\n",
    "    \"hbr\",\n",
    "]\n",
    "sfreq = 10.0  # in Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the data can be converted in to an MNE-Python data structure. The metadata above is used to create an `mne.Info` data structure, and this is combined with the data to create an MNE-Python\n",
    ":class:`mne.io.Raw` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = mne.create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sfreq)\n",
    "raw = mne.io.RawArray(data, info, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying standard sensor locations to imported data\n",
    "\n",
    "Having information about optode locations may assist in your analysis. Beyond the general benefits this provides (e.g. creating regions of interest, etc), this is may be particularly important for fNIRS as information about the optode locations is required to convert the optical density data in to an estimate of the haemoglobin concentrations. MNE-Python provides methods to load standard sensor configurations (montages) from some vendors, and this is demonstrated below.\n",
    "\n",
    "Below is an example of how to load the optode positions for an Artinis\n",
    "OctaMon device.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>It is also possible to create a custom montage from a file for\n",
    "          fNIRS with `mne.channels.read_custom_montage` by setting\n",
    "          `coord_frame` to `'mri'`.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage = mne.channels.make_standard_montage(\"artinis-octamon\")\n",
    "raw.set_montage(montage)\n",
    "\n",
    "# View the position of optodes in 2D to confirm the positions are correct.\n",
    "raw.plot_sensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the positions were loaded correctly it is also possible to view the location of the sources (red), detectors (black), and channels (white lines and orange dots) in a 3D representation. The ficiduals are marked in blue, green and red. See `Source alignment and coordinate frames` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_dir = mne.datasets.sample.data_path() / \"subjects\"\n",
    "mne.datasets.fetch_fsaverage(subjects_dir=subjects_dir)\n",
    "\n",
    "brain = mne.viz.Brain(\n",
    "    \"fsaverage\", subjects_dir=subjects_dir, alpha=0.5, cortex=\"low_contrast\"\n",
    ")\n",
    "brain.add_head()\n",
    "brain.add_sensors(raw.info, trans=\"fsaverage\")\n",
    "brain.show_view(azimuth=90, elevation=90, distance=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working woth CTF Data: the Brainstorm Auditory Dataset\n",
    "\n",
    "Here we compute the evoked from raw for the auditory Brainstorm\n",
    "tutorial dataset. For comparison, see the associated [brainstorm site](https://neuroimage.usc.edu/brainstorm/Tutorials/Auditory).\n",
    "\n",
    "Experiment:\n",
    "\n",
    "    - One subject, 2 acquisition runs 6 minutes each.\n",
    "    - Each run contains 200 regular beeps and 40 easy deviant beeps.\n",
    "    - Random ISI: between 0.7s and 1.7s seconds, uniformly distributed.\n",
    "    - Button pressed when detecting a deviant with the right index finger.\n",
    "\n",
    "The specifications of this dataset were discussed initially on the [FieldTrip bug tracker](http://bugzilla.fieldtriptoolbox.org/show_bug.cgi?id=2300)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mne\n",
    "from mne import combine_evoked\n",
    "from mne.datasets.brainstorm import bst_auditory\n",
    "from mne.io import read_raw_ctf\n",
    "from mne.minimum_norm import apply_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce memory consumption and running time, some of the steps are precomputed. To run everything from scratch change ``use_precomputed`` to ``False``. With ``use_precomputed = False`` running time of this script can be several minutes even on a fast computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_precomputed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was collected with a CTF 275 system at 2400 Hz and low-pass filtered at 600 Hz. Here the data and empty room data files are read to construct instances of `mne.io.Raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = bst_auditory.data_path()\n",
    "\n",
    "subject = \"bst_auditory\"\n",
    "subjects_dir = data_path / \"subjects\"\n",
    "\n",
    "raw_fname1 = data_path / \"MEG\" / subject / \"S01_AEF_20131218_01.ds\"\n",
    "raw_fname2 = data_path / \"MEG\" / subject / \"S01_AEF_20131218_02.ds\"\n",
    "erm_fname = data_path / \"MEG\" / subject / \"S01_Noise_20131218_01.ds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the memory saving mode we use ``preload=False`` and use the memory efficient IO which loads the data on demand. However, filtering and some other functions require the data to be preloaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = read_raw_ctf(raw_fname1)\n",
    "n_times_run1 = raw.n_times\n",
    "\n",
    "# Here we ignore that these have different device<->head transforms\n",
    "mne.io.concatenate_raws([raw, read_raw_ctf(raw_fname2)], on_mismatch=\"ignore\")\n",
    "raw_erm = read_raw_ctf(erm_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data array consists of 274 MEG axial gradiometers, 26 MEG reference\n",
    "sensors and 2 EEG electrodes (Cz and Pz). In addition:\n",
    "\n",
    "  - 1 stim channel for marking presentation times for the stimuli\n",
    "  - 1 audio channel for the sent signal\n",
    "  - 1 response channel for recording the button presses\n",
    "  - 1 ECG bipolar\n",
    "  - 2 EOG bipolar (vertical and horizontal)\n",
    "  - 12 head tracking channels\n",
    "  - 20 unused channels\n",
    "\n",
    "Notice also that the digitized electrode positions (stored in a .pos file)\n",
    "were automatically loaded and added to the `mne.io.Raw` object.\n",
    "\n",
    "The head tracking channels and the unused channels are marked as misc\n",
    "channels. Here we define the EOG and ECG channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.set_channel_types({\"HEOG\": \"eog\", \"VEOG\": \"eog\", \"ECG\": \"ecg\"})\n",
    "if not use_precomputed:\n",
    "    # Leave out the two EEG channels for easier computation of forward.\n",
    "    raw.pick([\"meg\", \"stim\", \"misc\", \"eog\", \"ecg\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For noise reduction, a set of bad segments have been identified and stored in csv files. The bad segments are later used to reject epochs that overlap with them. The file for the second run also contains some saccades. The saccades are removed by using SSP. We use pandas to read the data from the csv files. You can also view the files with your favorite text editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.DataFrame()\n",
    "offset = n_times_run1\n",
    "for idx in [1, 2]:\n",
    "    csv_fname = data_path / \"MEG\" / \"bst_auditory\" / f\"events_bad_0{idx}.csv\"\n",
    "    df = pd.read_csv(csv_fname, header=None, names=[\"onset\", \"duration\", \"id\", \"label\"])\n",
    "    print(f\"Events from run {idx}:\")\n",
    "    print(df)\n",
    "\n",
    "    df[\"onset\"] += offset * (idx - 1)\n",
    "    annotations_df = pd.concat([annotations_df, df], axis=0)\n",
    "\n",
    "saccades_events = df[df[\"label\"] == \"saccade\"].values[:, :3].astype(int)\n",
    "\n",
    "# Conversion from samples to times:\n",
    "onsets = annotations_df[\"onset\"].values / raw.info[\"sfreq\"]\n",
    "durations = annotations_df[\"duration\"].values / raw.info[\"sfreq\"]\n",
    "descriptions = annotations_df[\"label\"].values\n",
    "\n",
    "annotations = mne.Annotations(onsets, durations, descriptions)\n",
    "raw.set_annotations(annotations)\n",
    "del onsets, durations, descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the saccade and EOG projectors for magnetometers and add them to the raw data. The projectors are added to both runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saccade_epochs = mne.Epochs(\n",
    "    raw,\n",
    "    saccades_events,\n",
    "    1,\n",
    "    0.0,\n",
    "    0.5,\n",
    "    preload=True,\n",
    "    baseline=(None, None),\n",
    "    reject_by_annotation=False,\n",
    ")\n",
    "\n",
    "projs_saccade = mne.compute_proj_epochs(\n",
    "    saccade_epochs, n_mag=1, n_eeg=0, desc_prefix=\"saccade\"\n",
    ")\n",
    "if use_precomputed:\n",
    "    proj_fname = data_path / \"MEG\" / \"bst_auditory\" / \"bst_auditory-eog-proj.fif\"\n",
    "    projs_eog = mne.read_proj(proj_fname)[0]\n",
    "else:\n",
    "    projs_eog, _ = mne.preprocessing.compute_proj_eog(raw.load_data(), n_mag=1, n_eeg=0)\n",
    "raw.add_proj(projs_saccade)\n",
    "raw.add_proj(projs_eog)\n",
    "del saccade_epochs, saccades_events, projs_eog, projs_saccade  # To save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the effects of projections. Click on 'proj' button at the bottom right corner to toggle the projectors on/off. EOG events can be plotted by adding the event list as a keyword argument. As the bad segments and saccades were added as annotations to the raw data, they are plotted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical preprocessing step is the removal of power line artifact (50 Hz or 60 Hz). Here we notch filter the data at 60, 120 and 180 to remove the original 60 Hz artifact and the harmonics. The power spectra are plotted\n",
    "before and after the filtering to show the effect. The drop after 600 Hz appears because the data was filtered during the acquisition. In memory saving mode we do the filtering at evoked stage, which is not something you usually would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_precomputed:\n",
    "    raw.compute_psd(tmax=np.inf, picks=\"meg\").plot(\n",
    "        picks=\"data\", exclude=\"bads\", amplitude=False\n",
    "    )\n",
    "    notches = np.arange(60, 181, 60)\n",
    "    raw.notch_filter(notches, phase=\"zero-double\", fir_design=\"firwin2\")\n",
    "    raw.compute_psd(tmax=np.inf, picks=\"meg\").plot(\n",
    "        picks=\"data\", exclude=\"bads\", amplitude=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also lowpass filter the data at 100 Hz to remove the hf components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_precomputed:\n",
    "    raw.filter(\n",
    "        None,\n",
    "        100.0,\n",
    "        h_trans_bandwidth=0.5,\n",
    "        filter_length=\"10s\",\n",
    "        phase=\"zero-double\",\n",
    "        fir_design=\"firwin2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoching and averaging. First some parameters are defined and events extracted from the stimulus channel (UPPT001). The rejection thresholds are defined as peak-to-peak values and are in T / m for gradiometers, T for magnetometers and V for EOG and EEG channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.5\n",
    "event_id = dict(standard=1, deviant=2)\n",
    "reject = dict(mag=4e-12, eog=250e-6)\n",
    "# find events\n",
    "events = mne.find_events(raw, stim_channel=\"UPPT001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event timing is adjusted by comparing the trigger times on detected sound onsets on channel UADC001-4408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_data = raw[raw.ch_names.index(\"UADC001-4408\")][0][0]\n",
    "onsets = np.where(np.abs(sound_data) > 2.0 * np.std(sound_data))[0]\n",
    "min_diff = int(0.5 * raw.info[\"sfreq\"])\n",
    "diffs = np.concatenate([[min_diff + 1], np.diff(onsets)])\n",
    "onsets = onsets[diffs > min_diff]\n",
    "assert len(onsets) == len(events)\n",
    "diffs = 1000.0 * (events[:, 0] - onsets) / raw.info[\"sfreq\"]\n",
    "print(f\"Trigger delay removed (μ ± σ): {np.mean(diffs):0.1f} ± {np.std(diffs):0.1f} ms\")\n",
    "events[:, 0] = onsets\n",
    "del sound_data, diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mark a set of bad channels that seem noisier than others. This can also be done interactively with ``raw.plot`` by clicking the channel name (or the line). The marked channels are added as bad when the browser window is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.info[\"bads\"] = [\"MLO52-4408\", \"MRT51-4408\", \"MLO42-4408\", \"MLO43-4408\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epochs (trials) are created for MEG channels. First we find the picks for MEG and EOG channels. Then the epochs are constructed using these picks. The epochs overlapping with annotated bad segments are also rejected by default. To turn off rejection by bad segments (as was done earlier with saccades) you can use keyword ``reject_by_annotation=False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = mne.Epochs(\n",
    "    raw,\n",
    "    events,\n",
    "    event_id,\n",
    "    tmin,\n",
    "    tmax,\n",
    "    picks=[\"meg\", \"eog\"],\n",
    "    baseline=(None, 0),\n",
    "    reject=reject,\n",
    "    preload=False,\n",
    "    proj=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use first 40 good epochs from each run. Since we first drop the bad epochs, the indices of the epochs are no longer same as in the original epochs collection. Investigation of the event timings reveals that first epoch from the second run corresponds to index 182."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.drop_bad()\n",
    "\n",
    "# avoid warning about concatenating with annotations\n",
    "epochs.set_annotations(None)\n",
    "\n",
    "epochs_standard = mne.concatenate_epochs(\n",
    "    [epochs[\"standard\"][range(40)], epochs[\"standard\"][182:222]]\n",
    ")\n",
    "epochs_standard.load_data()  # Resampling to save memory.\n",
    "epochs_standard.resample(600, npad=\"auto\")\n",
    "epochs_deviant = epochs[\"deviant\"].load_data()\n",
    "epochs_deviant.resample(600, npad=\"auto\")\n",
    "del epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The averages for each conditions are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_std = epochs_standard.average()\n",
    "evoked_dev = epochs_deviant.average()\n",
    "del epochs_standard, epochs_deviant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical preprocessing step is the removal of power line artifact (50 Hz or 60 Hz). Here we lowpass filter the data at 40 Hz, which will remove all line artifacts (and high frequency information). Normally this would be done to raw data (with `mne.io.Raw.filter`), but to reduce memory consumption of this tutorial, we do it at evoked stage. (At the raw stage, you could alternatively notch filter with `mne.io.Raw.notch_filter`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evoked in (evoked_std, evoked_dev):\n",
    "    evoked.filter(l_freq=None, h_freq=40.0, fir_design=\"firwin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the ERF of standard and deviant conditions. In both conditions we can see the P50 and N100 responses. The mismatch negativity is visible only in the deviant condition around 100-200 ms. P200 is also visible around 170 ms in both conditions but much stronger in the standard condition. P300 is visible in deviant condition only (decision making in preparation of the button press). You can view the topographies from a certain time span by painting an area with clicking and holding the left mouse button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_std.plot(window_title=\"Standard\", gfp=True, time_unit=\"s\")\n",
    "evoked_dev.plot(window_title=\"Deviant\", gfp=True, time_unit=\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show activations as topography figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.arange(0.05, 0.301, 0.025)\n",
    "fig = evoked_std.plot_topomap(times=times)\n",
    "fig.suptitle(\"Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evoked_dev.plot_topomap(times=times)\n",
    "fig.suptitle(\"Deviant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the MMN effect more clearly by looking at the difference between the two conditions. P50 and N100 are no longer visible, but MMN/P200 and P300 are emphasised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_difference = combine_evoked([evoked_dev, evoked_std], weights=[1, -1])\n",
    "evoked_difference.plot(window_title=\"Difference\", gfp=True, time_unit=\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source estimation. We compute the noise covariance matrix from the empty room measurement and use it for the other runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject = dict(mag=4e-12)\n",
    "cov = mne.compute_raw_covariance(raw_erm, reject=reject)\n",
    "cov.plot(raw_erm.info)\n",
    "del raw_erm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation is read from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fname = data_path / \"MEG\" / \"bst_auditory\" / \"bst_auditory-trans.fif\"\n",
    "trans = mne.read_trans(trans_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time and memory, the forward solution is read from a file. Set ``use_precomputed=False`` in the beginning of this script to build the forward solution from scratch. The head surfaces for constructing a BEM solution are read from a file. Since the data only contains MEG channels, we only need the inner skull surface for making the forward solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_precomputed:\n",
    "    fwd_fname = data_path / \"MEG\" / \"bst_auditory\" / \"bst_auditory-meg-oct-6-fwd.fif\"\n",
    "    fwd = mne.read_forward_solution(fwd_fname)\n",
    "else:\n",
    "    src = mne.setup_source_space(\n",
    "        subject, spacing=\"ico4\", subjects_dir=subjects_dir, overwrite=True\n",
    "    )\n",
    "    model = mne.make_bem_model(\n",
    "        subject=subject, ico=4, conductivity=[0.3], subjects_dir=subjects_dir\n",
    "    )\n",
    "    bem = mne.make_bem_solution(model)\n",
    "    fwd = mne.make_forward_solution(evoked_std.info, trans=trans, src=src, bem=bem)\n",
    "\n",
    "inv = mne.minimum_norm.make_inverse_operator(evoked_std.info, fwd, cov)\n",
    "snr = 3.0\n",
    "lambda2 = 1.0 / snr**2\n",
    "del fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sources are computed using dSPM method and plotted on an inflated brain surface. For interactive controls over the image, use keyword ``time_viewer=True``. Standard condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_standard = mne.minimum_norm.apply_inverse(evoked_std, inv, lambda2, \"dSPM\")\n",
    "brain = stc_standard.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    subject=subject,\n",
    "    surface=\"inflated\",\n",
    "    time_viewer=False,\n",
    "    hemi=\"lh\",\n",
    "    initial_time=0.1,\n",
    "    time_unit=\"s\",\n",
    ")\n",
    "del stc_standard, brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviant condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_deviant = mne.minimum_norm.apply_inverse(evoked_dev, inv, lambda2, \"dSPM\")\n",
    "brain = stc_deviant.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    subject=subject,\n",
    "    surface=\"inflated\",\n",
    "    time_viewer=False,\n",
    "    hemi=\"lh\",\n",
    "    initial_time=0.1,\n",
    "    time_unit=\"s\",\n",
    ")\n",
    "del stc_deviant, brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_difference = apply_inverse(evoked_difference, inv, lambda2, \"dSPM\")\n",
    "brain = stc_difference.plot(\n",
    "    subjects_dir=subjects_dir,\n",
    "    subject=subject,\n",
    "    surface=\"inflated\",\n",
    "    time_viewer=False,\n",
    "    hemi=\"lh\",\n",
    "    initial_time=0.15,\n",
    "    time_unit=\"s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Importing Data from Eyetracking Devices\n",
    "\n",
    "Eyetracking devices record a persons point of gaze, usually in relation to a\n",
    "screen. Typically, gaze position (also referred to as eye or pupil position)\n",
    "and pupil size are recorded as separate channels. This section describes how to\n",
    "read data from supported eyetracking manufacturers.\n",
    "\n",
    "MNE-Python provides functions for reading eyetracking data. When possible,\n",
    "MNE-Python will internally convert and store eyetracking data according to an\n",
    "SI unit (for example radians for position data, and meters for pupil size).\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>If you have eye tracking data in a format that MNE does not support\n",
    "          yet, you can try reading it using other tools and create an MNE\n",
    "          object from a numpy array. Then you can use\n",
    "          :func:`mne.preprocessing.eyetracking.set_channel_types_eyetrack`\n",
    "          to assign the correct eyetrack channel types.</p></div>\n",
    "\n",
    "Some MNE functions may not be available to eyetracking and other\n",
    "             physiological data, because MNE does not consider them to be data\n",
    "             channels. See the `glossary` for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SR Research (Eyelink) (.asc)\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>MNE-Python currently only supports reading Eyelink eyetracking data\n",
    "          stored in the ASCII (.asc) format.</p></div>\n",
    "\n",
    "Eyelink recordings are stored in the Eyelink Data Format (EDF; .edf), which are binary files and thus relatively complex to support. To make the data in EDF files accessible, Eyelink provides the application EDF2ASC, which converts EDF files to a plain text ASCII format (.asc). These files can be imported into MNE using `mne.io.read_raw_eyelink`.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The Eyelink Data Format (EDF), should not be confused\n",
    "          with the European Data Format, the common EEG data format that also\n",
    "          uses the .edf extension.</p></div>\n",
    "\n",
    "Supported measurement types from Eyelink files include eye position, pupil size, saccadic velocity, resolution, and head position (for recordings collected in remote mode). Eyelink files often report ocular events (blinks, saccades, and fixations), MNE will store these events as `mne.Annotations`. Blink annotation descriptions will be ``'BAD_blink'``. For more information on the various measurement types that can be present in Eyelink files. read below.\n",
    "\n",
    "### Eye Position Data\n",
    "\n",
    "Eyelink samples can report eye position data in pixels, units of visual degrees, or as raw pupil coordinates. Samples are written as (x, y) coordinate pairs (or two pairs for binocular data). The type of position data present in an ASCII file will be detected automatically by MNE. The three types of position data are explained below.\n",
    "\n",
    "### Gaze\n",
    "\n",
    "Gaze position data report the estimated (x, y) pixel coordinates of the participants's gaze on the stimulus screen, compensating for head position changes and distance from  the screen. This datatype may be preferable if you are interested in knowing where the participant was looking at on the stimulus screen. The default (0, 0) location for Eyelink systems is at the top left of the screen.\n",
    "\n",
    "This may be best demonstrated with an example. In the file plotted below, eyetracking data was recorded while the participant read text on a display. In this file, as the participant read the each line from left to right, the x-coordinate increased. When the participant moved their gaze down to read a new line, the y-coordinate *increased*, which is why the ``ypos_right`` channel in the plot below increases over time (for example, at about 4-seconds, and at about 8-seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.datasets import misc\n",
    "from mne.io import read_raw_eyelink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpath = misc.data_path() / \"eyetracking\" / \"eyelink\"\n",
    "raw = read_raw_eyelink(fpath / \"px_textpage_ws.asc\", create_annotations=[\"blinks\"])\n",
    "custom_scalings = dict(eyegaze=1e3)\n",
    "raw.pick(picks=\"eyetrack\").plot(scalings=custom_scalings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**important**: \n",
    "\n",
    "```\n",
    "The (0, 0) pixel coordinates are at the top-left of the trackable area of the screen. Gaze towards lower areas of the screen will yield a relatively higher y-coordinate.\n",
    "```\n",
    "\n",
    "Note that we passed a custom `dict` to the ``'scalings'`` argument of\n",
    "`mne.io.Raw.plot`. This is because MNE's default plot scalings for eye\n",
    "position data are calibrated for HREF data, which are stored in radians\n",
    "(read below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head-Referenced Eye Angle (HREF)\n",
    "\n",
    "HREF position data measures eye rotation angles relative to the head. It does not take into account changes in subject head position and angle, or distance from the stimulus screen. This datatype might be preferable for analyses that are interested in eye movement velocities and amplitudes, or for simultaneous and EEG/MEG eyetracking recordings where eye position data are used to identify EOG artifacts.\n",
    "\n",
    "HREF coordinates are stored in the ASCII file as integer values, with 260 or more units per visual degree, however MNE will convert and store these coordinates in radians. The (0, 0) point of HREF data is arbitrary, as the relationship between the screen position and the coordinates changes as the subject's head moves.\n",
    "\n",
    "Below is the same text reading recording that we plotted above, except a new ASCII file was generated, this time using HREF eye position data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = misc.data_path() / \"eyetracking\" / \"eyelink\"\n",
    "raw = read_raw_eyelink(fpath / \"HREF_textpage_ws.asc\", create_annotations=[\"blinks\"])\n",
    "raw.pick(picks=\"eyetrack\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pupil Position\n",
    "\n",
    "Pupil position data contains (x, y) coordinate pairs from the eye camera. It has not been converted to pixels (gaze) or eye angles (HREF). Most use cases do not require this data type, and caution should be taken when analyzing raw pupil positions. Note that when plotting data from a ``Raw`` object containing raw pupil position data, the plot scalings will likely be incorrect. You can pass custom scalings into the ``scalings`` parameter of `mne.io.Raw.plot` so that the signals are legible when plotting.\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>If a calibration was not performed prior to data collection, the\n",
    "             EyeLink system cannot convert raw pupil position data to pixels\n",
    "             (gaze) or eye angle (HREF).</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pupil Size Data\n",
    "\n",
    "Pupil size is measured by the EyeLink system at up to 500 samples per second. It may be reported as pupil *area*, or pupil *diameter* (i.e. the diameter of a circle/ellipse model fit to the pupil area). Which of these datatypes you get is specified by your recording- and/or your EDF2ASC settings. The pupil size data is not calibrated and reported in arbitrary units. Typical pupil *area* data range between 800 to 2000 units, with a precision of 1 unit, while pupil *diameter* data range between 1800-3000 units.\n",
    "\n",
    "### Velocity, resolution, and head position data\n",
    "\n",
    "Eyelink files can produce data on saccadic velocity, resolution, and head position for each sample in the file. MNE will read in these data if they are present in the file, but will label their channel types as ``'misc'``.\n",
    "\n",
    "**Warning**:\n",
    "```\n",
    "Eyelink's EDF2ASC API allows for modification of the data and format that is converted to ASCII. However, MNE-Python assumes a specific structure, which the default parameters of EDF2ASC follow. ASCII files should be tab-deliminted, and both Samples and Events should be output. If the data were recorded at 2000Hz, timestamps should be floating point numbers. Manual modification of ASCII conversion via EDF2ASC is not recommended.       \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
